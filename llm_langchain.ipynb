{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Instantiate an instance of the local Ollama model\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capitals are:\n",
      "\n",
      "1. **France**: Paris\n",
      "2. **Belgium**: Brussels\n",
      "3. **China**: Beijing (Note: China has a complex administrative structure with multiple capital cities for different levels of government. However, Beijing is the capital of the People's Republic of China and the country as a whole.)\n"
     ]
    }
   ],
   "source": [
    "# In this case it's a human message, but this can change \n",
    "prompt = [HumanMessage('What is the Capital of France, Belgium and China?')]\n",
    "completion = llm.invoke(prompt)\n",
    "print(completion.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The' additional_kwargs={} response_metadata={} id='run-1ee9a45a-1110-4f8e-b7e4-677d66b3de7e'\n",
      "content=' capital' additional_kwargs={} response_metadata={} id='run-1ee9a45a-1110-4f8e-b7e4-677d66b3de7e'\n",
      "content=' of' additional_kwargs={} response_metadata={} id='run-1ee9a45a-1110-4f8e-b7e4-677d66b3de7e'\n",
      "content=' France' additional_kwargs={} response_metadata={} id='run-1ee9a45a-1110-4f8e-b7e4-677d66b3de7e'\n",
      "content=' is' additional_kwargs={} response_metadata={} id='run-1ee9a45a-1110-4f8e-b7e4-677d66b3de7e'\n",
      "content=' Paris' additional_kwargs={} response_metadata={} id='run-1ee9a45a-1110-4f8e-b7e4-677d66b3de7e'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run-1ee9a45a-1110-4f8e-b7e4-677d66b3de7e'\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2024-11-10T18:38:45.219663Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 550681208, 'load_duration': 16288750, 'prompt_eval_count': 17, 'prompt_eval_duration': 156000000, 'eval_count': 8, 'eval_duration': 377000000} id='run-1ee9a45a-1110-4f8e-b7e4-677d66b3de7e' usage_metadata={'input_tokens': 17, 'output_tokens': 8, 'total_tokens': 25}\n"
     ]
    }
   ],
   "source": [
    "# Streaming the same message \n",
    "# In this case it's a human message, but this can change \n",
    "messages = [\n",
    "(\"human\", \"What is the Capital of France?\")\n",
    "]\n",
    "\n",
    "\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "\n",
    "completion = chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Programmieren ist mir ein Leidenschaft! (That's \"Programming is my passion!\" in German.) Would you like me to translate anything else?\n"
     ]
    }
   ],
   "source": [
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks.manager import CallbackManager\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(callbacks=<langchain_core.callbacks.manager.CallbackManager object at 0x1070494b0>, model='llama3.1', temperature=0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is callbacks doing here?\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) \n",
    "    # other params...\n",
    ")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Tenzing Norgay and Sir Edmund Hillary!\n",
      "\n",
      "On May 29, 1953, Tenzing Norgay, a Nepali Sherpa mountaineer, and Sir Edmund Hillary, a New Zealand mountaineer and explorer, became the first two people to reach the summit of Mount Everest, which stands at an elevation of 8,848 meters (29,029 feet) above sea level.\n",
      "\n",
      "Their historic achievement was part of a British expedition led by John Hunt. Tenzing and Hillary's successful ascent marked a major milestone in mountaineering history!...Tenzing Norgay and Sir Edmund Hillary!\n",
      "\n",
      "On May 29, 1953, Tenzing Norgay, a Nepali Sherpa mountaineer, and Sir Edmund Hillary, a New Zealand mountaineer and explorer, became the first two people to reach the summit of Mount Everest, which stands at an elevation of 8,848 meters (29,029 feet) above sea level.\n",
      "\n",
      "Their historic achievement was part of a British expedition led by John Hunt. Tenzing and Hillary's successful ascent marked a major milestone in mountaineering history!\n"
     ]
    }
   ],
   "source": [
    "# In this case it's a human message, but this can change \n",
    "prompt = [HumanMessage(\"The first man on the summit of Mount Everest, the highest peak on Earth, was ...\")]\n",
    "completion = llm.invoke(prompt)\n",
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from fastapi import FastAPI\n",
    "from langchain.llms import Ollama\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langserve import add_routes\n",
    "import uvicorn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rz/v2_v6xwd24v6fdtmrs313wmr0000gn/T/ipykernel_12512/1459776103.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llama3 = Ollama(model=\"llama3.1\")\n"
     ]
    }
   ],
   "source": [
    "llama3 = Ollama(model=\"llama3.1\")\n",
    "template = PromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "chain = template | llama3 | CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from fastapi import FastAPI\n",
    "from langchain.llms import Ollama\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langserve import add_routes\n",
    "import uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply nest_asyncio to allow Uvicorn to run in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FastAPI app and LangChain setup\n",
    "app = FastAPI(title=\"LangChain\", version=\"1.0\", description=\"The first server ever!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rz/v2_v6xwd24v6fdtmrs313wmr0000gn/T/ipykernel_13197/448717468.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llama3 = Ollama(model=\"llama3.1\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize LangChain components\n",
    "llama3 = Ollama(model=\"llama3.1\")\n",
    "template = PromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "chain = template | llama3 | CommaSeparatedListOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the route setup function\n",
    "def setup_routes():\n",
    "    # Check if the '/chain' route already exists to prevent duplicates\n",
    "    if not any(route.path == \"/chain\" for route in app.routes):\n",
    "        add_routes(app, chain, path=\"/chain\")\n",
    "    else:\n",
    "        print(\"Route '/chain' already exists, skipping setup.\")\n",
    "\n",
    "# Run the setup function\n",
    "setup_routes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic test route\n",
    "@app.get(\"/\")\n",
    "async def read_root():\n",
    "    return {\"message\": \"Hello from FastAPI in Jupyter!\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [13197]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:8001 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
      "    |  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
      "    |  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
      "    |  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
      "    |  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
      "    |_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
      "    \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/chain/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  â\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  âââ> /chain/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
      "INFO:     ::1:54000 - \"GET /chain/playground/ HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:54000 - \"GET /chain/playground/assets/index-400979f0.js HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:54001 - \"GET /chain/playground/assets/index-52e8ab2f.css HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:54000 - \"GET /chain/playground/favicon.ico HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:54000 - \"POST /chain/stream_log HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    ASGI callable returned without completing response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:54005 - \"POST /chain/stream_log HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    ASGI callable returned without completing response.\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [13197]\n"
     ]
    }
   ],
   "source": [
    "# Start Uvicorn server in Jupyter\n",
    "\n",
    "#Â Copy and paste this: http://localhost:8000/chain/playground/\n",
    "uvicorn.run(app, host=\"localhost\", port=8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
